---
title: "STATS 551 - HW4"
author: "Zhen Qin"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 1.

### 1.

For ordinary linear model, $(y_i|\beta_i,X)\sim N(\beta_1x_{i1}+...+\beta_k x_{ik},\sigma^2)$, $p(\beta,\sigma^2|X)\propto \sigma^{-2}$. According to the theory, the rank of predictors is 9, $n=16>9$. The posterior is $(\beta|\sigma,y)\sim N(\hat{\beta},V_{\beta}\sigma^2),(\sigma^2|y)\sim inv-\chi^2(5,s^2)$

```{r}
y=c(49,50.2,50.5,48.5,47.5,44.5,28,31.5,34.5,35,38,38.5,15,17,20.5,19.5)
x1=scale(c(rep(1300,6),rep(1200,6),rep(1100,4)))
x2=scale(c(7.5,9,11,13.5,17,23,5.3,7.5,11,13.5,17,23,5.3,7.5,11,17))
x3=scale(c(0.012,0.012,0.0115,0.013,0.0135,0.012,0.04,0.038,0.032,0.026,0.034,0.041,0.084,0.098,0.092,0.086))
dat =data.frame(y=y,x1=x1,x2=x2,x3=x3)
dat$x12 = dat$x1*dat$x2
dat$x13 = dat$x1*dat$x3
dat$x23 = dat$x2*dat$x3
dat$x11 = dat$x1^2
dat$x22 = dat$x2^2
dat$x33 = dat$x3^3

lr = lm(y~.,dat)
summary(lr)
plot(lr$fitted.values,y)
lines(quantile(y, c(0.01, 0.99)), quantile(y, c(0.01, 0.99)), col = "red")
X=as.matrix(dat[,-1])
solve(t(X)%*%X)
```

### 2.

For a mixed-effects model, $p(\beta_0)\propto 1$, $\beta_i\sim N(\mu,\sigma^2), i=1,...,9$. 

```{r}
library(rstan)
library(ggplot2)
library(StanHeaders)
hlr =stan(file = "hlr.stan", data =list(N =length(y), y = dat$y, x = dat[,-1],
                                       tau =1,iter = 1000, chains = 4, refresh = 0))
traceplot(hlr, pars =names(hlr)[1:12])
hlrfit = extract(hlr, permuted = TRUE)
```

### 3.

In question 2, all coefficients are around 0. In question 1, the variance matrix of beta is big, so the results will be less stable. Their conclusion is not acceptable.

### 4.

```{r}
t4lr =stan(file = "t4lr.stan", data =list(N =length(y), y = dat$y, x = dat[,-1],
                                        iter = 1000, chains = 4, refresh = 0))
traceplot(t4lr, pars =names(t4lr)[1:12])
```

### 5.

Here I use another heavy tailed distribution, student_t(10). This result is similar with question 4.

```{r}
tlr =stan(file = "tlr.stan", data =list(N =length(y), y = dat$y, x = dat[,-1],
                                        iter = 1000, chains = 4, refresh = 0))
traceplot(tlr, pars =names(tlr)[1:10])
```

## 2.

### 1.

Fit a regression model using the g-prior. Posterior confidence intervals are shown below.

```{r}
az = read.table('azdiabetes.dat',header = T)
az = az[,-8]
az[,-2] = scale(az[,-2])
X = as.matrix(az[,-2])
xtx = X%*%solve(t(X)%*%X)%*%t(X)
```

```{r}
library(MASS)
pos = function(y,x,g,nu_0,sig_0,S)
{
  
  n = dim(x)[1]; p = dim(X)[2]
  Hg = (g/(g+1))*(x%*%solve(t(x)%*%x)%*%t(x))
  SSRg = t(y)%*%(diag(1,nrow = n) - Hg)%*%y
  sig = 1/rgamma(S,(nu_0+n)/2,(nu_0*sig_0 + SSRg)/2)
  Vb <-  g*solve(t(X)%*%X)/(g+1)
  Eb <- Vb%*%t(X)%*%y
  E <- matrix (rnorm(S*p , 0 , sqrt(sig)),S,p)
  beta <- t(t(E%*%chol(Vb))+c(Eb))
  return(list(beta = beta, sigsq = sig))
  
}
y = as.vector(az[,2]); X = as.matrix(az[,-2]);S = 1000
m1 = pos(y,X,532,2,1,S)
ts.plot(m1$sigsq,xlab = "samples")

mean(m1$sigsq)
par(mfrow = c(3,2))
for(i in 1:6)
{
  ts.plot(m1$beta[,i], xlab = "samples", ylab = paste("beta",i))
}
colMeans(m1$beta)
std = apply(m1$beta,2,sd)
posin = apply(m1$beta,2,function(x)quantile(x,c(0.025,.975)))
posin
```

### 2.

```{r, eval=FALSE, echo=TRUE}
lpy.X <- function( y ,X, g=length( y ),
                          nu_0=2, sig_0=1)
{
  n <- dim(X)[1] ; p <- dim(X)[2]
  if ( p==0) { Hg <- 0 }
  if(p>0){
    Hg = (g /( g+1))*X%*%solve(t(X)%*%X)%*%t(X)
  }
  SSRg <-  t(y)%*%( diag(1,nrow=n )- Hg)%*%y 
  ans = -.5*( n* log(pi)+p*log(1+g)+(nu_0+n)*log(nu_0* sig_0+SSRg)-
        nu_0*log(nu_0* sig_0)) +  lgamma (( nu_0+n ) / 2 ) - lgamma ( nu_0 / 2 )
  return(ans)
}
#####
##### starting values and MCMC setup
z <- rep ( 1 , dim(X)[2])
lpy.c <- lpy.X(y,X[ , z==1,drop=FALSE] )
S <- 1000
Z <- matrix (NA, S , dim(X)[2])
B = matrix (NA, S , dim(X)[2])
sigsq = rep(NA,S)
#####
##### Gibbs s ample r
for ( s in 1:S )
{
  for ( j in sample ( 1:dim(X)[2]))
  {
    zp <- z ; zp [j] <- 1-zp[j]
    lpy.p <- lpy.X( y ,X[, zp==1,drop=FALSE] )
    r <-  ( lpy.p - lpy.c )*( -1)^( zp[j]==0)
    z [ j ] <- rbinom ( 1 , 1 , 1/( 1+ exp(-r ) ) )
    if ( z [j]==zp [ j ] ) { lpy.c <- lpy.p}
  }
  Z [ s ,] <- z
  m = pos(y,X[,z==1],length(y),nu_0 = 2,sig_0 = 1,S = 1)
  B[s,] = m$beta; sigsq[s] = m$sigsq
  
}
```

I compare the results with averaging procedures. It is clear that by P. Hoff's method the variance of npreg increases and others are very similar.

```{r}
load('.RData')
prob = colMeans(Z); nm = colnames(az); nm = nm[-2]; cbind(nm,prob)
for(i in 1:6)
{
  hist(B[,i], xlab = paste("beta",i),main = nm[i])
}
posin2 = apply(B,2,function(x)quantile(x,c(0.025,.975)))
colnames(posin2) = nm
colnames(posin) = nm
posin2
```